---
title: "Example Notebook to Analyse and Visualize Data from a Micro Scenario-based Study"
author:
  - name: "Philipp Brauner"
    orcid: 0000-0003-2837-5181
    affiliation: 
      - name: RWTH Aachen University, Communication Science
        city: Aachen
        country: Germany
        url: www.comm.rwth-aachen.de
date-format: long
doi: "not yet"
title-block-banner: "true"
abstract: > 
  This R notebook serves as an example, demonstrating the analysis and visualization of micro-scenario-based studies. Micro-scenarios provide an approach for evaluating the social acceptance of technologies and the determining factors, along with a visual-spatial mapping of the results. They enable the simultaneous assessment of multiple technologies, ranking them based on different criteria, and analyzing how individual factors and technology-based attributions correlate with the overall assessment of technologies. Utilizing synthetic survey data (generated in a separate notebook), this notebook illustrates how to recode the data, aggregate scenario scores as user factors, calculate topic scores, and visualize them using the R programming language, along with ggplot and tidyverse.
keywords:
  - Micro scenarios
  - Topic Mapping
  - R
funding: "Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy — EXC-2023 Internet of Production — 390621612."
editor: 
  markdown: 
    wrap: sentence
---

# Introduction

The micro-scenario approach simplifies measuring people's opinions on different topics.
It connects these opinions to individual user factors (research perspective 1), ranks the topics, and creates a visual map to pinpoint conflicting issues (research perspective 2), all within a single survey.

For instance, consider analysing risk-utility trade-offs among various technologies:
Do individuals attribute varying risks and utilities to distinct technologies?
Are people predisposed to different risk or utility perceptions?
Is the comparability of risk-utility trade-offs consistent across different technologies, and can these trade-offs be quantified?
Figure @fig-concept illustrates the overall approach.

![The micro-scenario approach involves consolidating evaluations of diverse topics in a single survey. These evaluations are treated as topic assessments and spatially mapped to analyze the interrelationships among them.](figures/concept.png){fig-align="center" #fig-concept}

The main article provides comprehensive insights into this approach and outlines the methodology for designing and analyzing studies. You can locate and cite the main article here: [insert citation information].

-   Brauner, P. et al. (2024), Visual Maps of Technology Acceptance -- Spatial Mapping of Factors Influencing Social Acceptance of Technologies Using Micro Scenarios, XXX

This notebook demonstrates the calculation of data for the two research perspectives of the micro-scenario approach (Perspective 1: user factor and Perspective 2: topic factor) using R.
Note that all transformations and calculations can also be performed using other software.

In this example, we utilize synthetic data generated to resemble real survey data.
This choice simplifies the follow-through of our approach, eliminating the need for cleaning the data from irrelevant variables or erroneous participant inputs.
Additionally, the synthetic data adheres to pre-specified properties.
The creation of the synthetic data is detailed in the companion notebook within the same folder.

The rest of this notebook is organized as follows:
Firstly, we load the necessary packages, followed by the creation of synthetic data for our input (replace this with your actual data).
Secondly, we transform the data into the long format (refer to, for instance, <https://tidyr.tidyverse.org/reference/pivot_longer.html>), proceed to analyse the data as a user factor (research perspective 1), and subsequently as a topic factor which includes visualizing the outcomes (research perspective 2). 

# Preparation

## Load required libraries

In our analysis, we mainly use the `tidyverse` and `ggplot` packages.
To generate synthetic data, we also employ the `faux` package, which facilitates data generation with specific properties, such as pre-defined means and correlations between variables (see second notebook).

```{r LOADLIBRARIES}
library(tidyverse)
library(scales)  # format_percent
library(ggplot2) # graphics
library(ggrepel) # label placement in the scatter plot
library(knitr)   # Tables
```

## Load Data

In this demonstration, we will load synthetic data that emulates the properties found in real survey data.
Figure @fig-surveysdatastructure illustrates the structure of a standard dataset from survey tools, where each row represents the responses from an individual participant.

![Illustration of typical survey data utilizing the micro-scenario approach, featuring user demographics, additional user factors, and topic evaluations.](figures/surveydata.png){fig-align="center" #fig-surveysdatastructure}

The data structure closely resembles the data export from the Qualtrics survey tool.
The process of generating synthetic data is documented in the companion notebook within the same folder.

```{r LOADDATA}
  data <- readRDS("syntheticdata.rds")
```

The structure of the data looks as follows:

```{r PRINTSTRUCTURE, echo=FALSE}
  str(data)
```

The loaded dataset has various variables.
Initially, there's a unique user identifier (`id`), followed by a user variable (e.g., attitude towards a topic).
Subsequently, there are an arbitrary number of topic assessments (*N* in our example) with variables for each evaluation dimension.
In this instance, we use *perceived risk* and *perceived utility* as examples for the topic evaluations. However, one can employ different or additional evaluation dimensions (as detailed in the article).

The variables for the topic evaluations adhere to a standardized naming scheme, i.e., `a01_matrix_02`, where `01` denotes the ID of the queried topic, `02` represents the queried evaluation dimension, and `matrix` stands for the name of the variable block in the survey tool.
This naming scheme is employed by Quartics.

# Analysis of the data

Once the (synthetic) survey data is loaded into the variable `data`, we can commence the actual analysis.

## Setup

Firstly, read the list of queried topics and their labels from a `.csv` file (adjustable based on your needs).
Secondly, define the queried evaluation dimensions.
In this instance, we have a vector of two dimensions, but one can define more based on your research questions and survey structure.

```{r LOADLABELS}
TOPICS <- read.csv2("matrixlabels.csv") 
DIMENSIONS = c("risk", "utility")  
```

## Long Format

Next, the topic evaluations from the survey data is transformed into the long format using `pivot_longer` (one row with a single value for each participant, topic, and evaluation dimension; one row per observation). Hereto, we use that the variables for the topic evaluations in the original data table have a systematic naming convention (see above).

The resulting data set contains a participant identifier, identifier for the topic and the evaluation dimension, and lastly a column for the value. We use this format as the foundation for the later transformation steps.

```{r SURVEYTOLONG}
evaluationsLong <- data %>% 
  # selects columns id and "aNUMBER_matrix_NUMBER" (scheme from loop&merge)
  dplyr::select(id, matches("a\\d+\\_matrix\\_\\d+")) %>%
  tidyr::pivot_longer(
    cols = matches("a\\d+\\_matrix\\_\\d+"), # topics and their evaluations
    names_to = c("question", "dimension"),
    names_pattern = "(.*)_matrix_(.*)",   # Separate topic ID and evaluation ID 
    values_to = "value",
    values_drop_na = FALSE) %>%
    dplyr::mutate( dimension = as.numeric(dimension) ) %>%
    dplyr::mutate( dimension = DIMENSIONS[dimension]) %>%  # change to readable dimension names
    dplyr::mutate( value = -(((value - 1)/3) - 1))  # rescale value from [ 1...7 ] to [ -100%...100% ]

# Recode some of the evaluation dimensions if necessary
evaluationsLong <- evaluationsLong %>%
  dplyr::mutate( value = if_else(dimension!="risk", value, -value))
```

## Perspective 1: As user factor

The initial perspective provides a straightforward view of the data.
The different presented scenarios serve as a basis for the repeated measurement of the same latent construct and the resulting score can be regarded as a user factor (or individual differences).
For each evaluation dimension (e.g., *risk* and *utility*), we compute average scores across all queried topics.
Using these cores one can, for instance, investigate if the overall attributions differ among participants or if they correlate with other queried user factors.
For example, exploring if the average risk attributed to all topics relates to a general disposition to risk measured using other psychometric scales.

Subsequently, we rejoin these user factors with the original data using, for instance, `dplyr::left_join()`.
Afterwards, the calculated average evaluations can be regarded as individual differences and correlated with other user factors obtained from the survey.

```{r PERSPECTIVE1_USERFACTOR}
evaluationByParticipant <- evaluationsLong %>%
  tidyr::pivot_wider(names_from = dimension, values_from = value) %>%
  dplyr::group_by(id) %>%
  dplyr::summarize(
    across(
      all_of( DIMENSIONS ),  # Select only evaluation dimensions
      list( mean = ~mean(., na.rm = TRUE),
#            median = ~median(., na.rm = TRUE),
            sd = ~sd(., na.rm = TRUE)),
      .names = "{.col}_{.fn}"  # Scheme to define column names
    ), .groups="drop"
  ) %>%
  dplyr::left_join(data, by="id")
```

#### Example research questions:

How is the average perceived *risk* of the participants?

```{r PERSPECTIVE1_VARIABLESUMMARYRISK}
summary(evaluationByParticipant$risk_mean)
```

How is the average perceived *utility* of the participants?

```{r PERSPECTIVE1_VARIABLESUMMARYUZTILITY}
summary(evaluationByParticipant$utility_mean)
```

Does *uservariable* from the survey correlate to *risk*?

```{r PERSPECTIVE1_USERVARIABLERISK}
cor.test(evaluationByParticipant$risk_mean,
    evaluationByParticipant$uservariable)
```

Does uservariable correlate with the average *perceived utility*?

```{r PERSPECTIVE1_USERVARIABLEUTILITY}
cor.test(evaluationByParticipant$utility_mean,
    evaluationByParticipant$uservariable)
```

## Perspective 2: Topic factors

TODO: WARUM BRAUCHE ICH DAS.

[ ] Konstentenz: Wechselt von Valenz und Expectancy. auf Risk und Utility.
=> Besser nur Risk und Utility.

[ ] Rustikale Grafik 3.

[ ] Median weglassen. Kommt plötzlich und nur einmal.

[ ] (outlier) -> (absichtlicher deliberate outlier)

[ ] Regressionsgrade hinter die Punkte.

[ ] darstellungsorm erinnert an das kanu-modell - basismerkmal, 


Next, we delve into the analysis of topic evaluations.
We start with reporting the average evaluations (e.g., *risk* and *utility*) across all queried topics.

### Calculate Average evaluations

Using the long format, we group by evaluation dimension and aggregate across all topics and participants.
Note: For a complete sample, the results are equivalent to perspective 1 (see above).
Tables @tbl-dimensions and Figure @fig-evaluations depict the outcome of this calculation.

```{r PERSPECTIVE2_OVERVIEW}
# MEAN and SD of all evaluation dimensions across all queried topics
evaluationByDimension <- evaluationsLong %>%
  dplyr::group_by( dimension ) %>%
  dplyr::summarise( mean = mean(value, na.rm = TRUE),
                    sd = sd(value, na.rm = TRUE),
                    .groups="drop")
```

```{r TABLEDIMENSIONS, echo=FALSE}
#| label: tbl-dimensions
#| tbl-cap: Averages for each evaluation dimension across all queried topics and across all participants.
knitr::kable(evaluationByDimension)
```

```{r PERSPECTIVE2_PLOTOVERVIEW}
#| label: fig-evaluations
#| fig-cap: "Mean evaluation across all topics and aggregated across all participants."
overallDimension <- ggplot(evaluationByDimension,
  aes(x = dimension, y = mean)) + 
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = percent_format(),
                     limits=c( -1, +1 )) +
  labs(x = "Evaluation Dimension",
       y = "Values",
       title = "Average Evaluation across all Dimensions and Participants")
overallDimension
```

### Prepare Individual Topics

Now, we compute the average evaluations for each topic across all participants.
The resulting data frame contains N rows for the number of topics queried and rows for the arithmetic mean and standard deviation for each evaluated dimension (e.g., *risk* and *utility*).
Finally, we associate labels with each topic using `dplyr::left_join()`.
Figure @fig-topicdatastructure illustrates the structure of the resulting data.

![The resulting data format displays the evaluation of topics. Each row contains the mean evaluation (along with its dispersion) for a specific topic. This structured data can be subjected to further analysis.](figures/topicdata.png){fig-align="center" #fig-topicdatastructure}

The output can be tabulated, sorted, or filtered based on highest/lowest evaluations, and visualized.
Table @tbl-topics displays the unsorted and unfiltered results.


```{r PERSPECTIVE2_TOPICFACTOR}
evaluationByTopic <-  evaluationsLong %>%
  tidyr::pivot_wider(
    names_from = dimension,
    values_from = value) %>%
  dplyr::group_by( question ) %>%
  dplyr::summarize(
    across(
      all_of( DIMENSIONS ),  # Select the variables from var_names
      list(mean = ~mean(., na.rm = TRUE),
      	   sd = ~sd(., na.rm = TRUE)),
      .names = "{.col}_{.fn}"    # Define column names for the results
    ), .groups="drop"
  ) %>%
  dplyr::left_join(TOPICS, by="question") # attach question labels
```

```{r TABLETOPICS, echo=FALSE}
#| label: tbl-topics
#| tbl-cap: Average evaluation of the queried topics.
knitr::kable(
  evaluationByTopic %>%
    dplyr::select(-question, -shortlabel) %>%
    dplyr::relocate(label))
```

### Topic Correlations

Next, we analyse the correlation between the evaluation dimensions across the topics.
In the example in Table @tbl-topiccorrelations, we investigate if the attribute *risk* is related to the attributed *utility* for the different topics under consideration.
In this example, we have only two target variables for the topic evaluations.
With more variables, more complex analyses become possible: Such as determining if and to what degree a linear model with *risk* and *utility* explains the overall *valence* towards the queried topics.

Note: Our analysis focuses on the correlations between the topics as attributed by the participants, rather than individual differences among participants.

```{r TOPICCORRELATIONS}
#| label: tbl-topiccorrelations
#| tbl-cap: Correlations between the evaluation dimensions across all topics
evaluationByTopic %>%
  dplyr::select(ends_with("_mean")) %>%
  correlation::correlation() %>%
 kable()
```

### Visualize the Topics

Finally, the results are presented through a scatter plot.
The plot in Figure @fig-topicscatterplot allows for the visual identification of the dispersion of topics on a spatial map defined by the evaluation dimension.
It helps assess if there is a (linear) relationship between the queried evaluation dimensions of the topics, the slope and intercept of that relationship, and if some topics exhibit significantly different evaluations compared to others (outliers).

```{r PERSPECTIVE2_EXAMPLEPLOT}
#| label: fig-topicscatterplot
#| fig-cap: "Scatter plot of the evaluations of the micro scenarios."
#| warning: false

scatterPlot <- evaluationByTopic %>%
  ggplot( aes( x = risk_mean,
               y = utility_mean,
               label = shortlabel)) + 
  coord_cartesian(clip = "on") +
  geom_vline(xintercept = 0, size = 0.25, color="black", linetype=1) + 
  geom_hline(yintercept = 0, size = 0.25, color="black", linetype=1) + 
  # diagonal line indicating where both dimensions are congruent 
  annotate("segment",
           x = -1, y = +1,
           xend = +1, yend = -1,
           colour = "black",
           linewidth = 0.25,
           linetype = 2) +
  # Annotate the quadrants
  geom_label(aes(x = -1, y = -1, label = "LOW RISK & LOW UTILITY"),
             vjust = "middle", hjust = "inward",
             size = 1.75,
             label.size = NA, color="black", fill = "#c7ddf2") +
  geom_label(aes(x = -1, y = +1, label = "LOW RISK & HIGH UTILITY"),
             vjust = "middle", hjust = "inward",
             size = 1.75,
             label.size = NA, color="black", fill = "#c7ddf2") + 
  geom_label(aes(x = +1, y = -1, label = "HIGH RISK & LOW UTILITY"),
             vjust = "middle", hjust = "inward",
             size = 1.75,
             label.size = NA, color="black", fill = "#c7ddf2") + 
  geom_label(aes(x = +1, y = +1, label = "HIGH RISK & HIGH UTILITY"),
             vjust = "middle", hjust = "inward",
             size = 1.75,
             label.size = NA, color="black", fill = "#c7ddf2") + 
  # add the labels...
  geom_label_repel(
    max.time = 3,
    color = "black",
    fill = "gray95",
    force_pull   = 0,
    max.overlaps = Inf,
    ylim = c(-Inf, Inf),
    xlim = c(-Inf, Inf),
    segment.color ="#00549f",
    segment.size = 0.25,
    min.segment.length = 0,
    size = 2.5,
    label.size = NA,
    label.padding = 0.105,
    box.padding = 0.125
  ) +
  geom_smooth(method = "lm", se = TRUE) +
  geom_point() +      # geom for the data points
  labs( title = "Illustration of the risk-utility tradeoff ...",
        caption = "Based on synthetic data for illustrative purposes. See linked companion notebook under https://osf.io/96ep5/",
        x = "AVERAGE ESTIMATED RISK\n(without risk — very risky)",
        y = "AVERAGE ESTIMATED UTILITY\n(useless — useful)") +
  scale_x_continuous(labels = percent_format(), limits=c( -1, +1 )) +
  scale_y_continuous(labels = percent_format(), limits=c( -1, +1 ))
scatterPlot

ggsave("simulatedriskutility.pdf",
       plot = scatterPlot,
       width = 8, height = 6,
       units = "in")
```

# Closing remarks

This notebook showcases the analysis and visualization of surveys using the micro-scenario approach.
It includes executable code for examining both research perspectives (individual differences and topic evaluation), which can be adjusted to suit your own survey and data.
Ensure accurate coding and polarization of input variables.

It is crucial to recognize the limitations of this approach (e.g., when point estimations are acceptable, potential bias from topic sampling), and refer to the main articles for further guidance and strategies for mitigation.


# Acknowledgements

This approach has evolved through an extended period and across numerous research projects. 
We extend our gratitude to all who have, directly or indirectly, consciously or unconsciously, inspired us to delve into this approach and provided opportunities for its application in diverse contexts.
Special thanks to Julian Hildebrandt for feedback on an earlier version of this notebook.

Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy — EXC- 2023 Internet of Production — 390621612.
