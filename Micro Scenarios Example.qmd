---
title: "Example Notebook to Analyse and Visualize Data from a Micro Scenario-based Study"
author:
  - name: "Philipp Brauner"
    orcid: 0000-0003-2837-5181
    affiliation: 
      - name: RWTH Aachen University, Communication Science
        city: Aachen
        country: Germany
        url: www.comm.rwth-aachen.de
date-format: long
doi: "not yet"
title-block-banner: "true"
abstract: > 
  This is an example R notebook that demonstrates the analysis and visualisation of micro-scenario based studies. Micro-scenarios are an approach for assessing the social acceptance of technologies and the factors that determine it, and a visual-spatial mapping of the results. They facilitate the assessment of many technologies in parallel, the ranking of technologies based on different criteria, and the analysis of how individual factors and technology-based attributions relate to the overall assessment of technologies. Based on synthetic survey data (generated in a separate notebook), the notebook illustrates how to re-code the data, how to aggregate scenario scores as user factors, how to calculate topic scores, and how to visualise them using the programming language R, ggplot and tidyverse.
keywords:
  - Micro scenarios
  - Topic Mapping
  - R
funding: "Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy — EXC-2023 Internet of Production — 390621612."
editor: 
  markdown: 
    wrap: sentence
---

# Introduction

The micro scenario approach facilitates measuring people's attributions towards various topics, and relating them to individual user factors (research perspective 1), a ranking of these topics, and a visio-spatial mapping to identify conflicting issues (research perspective 2) in a single survey.

An example may be the analysis of the risk-utility trade-offs between different technologies: Do people have different risk or utility dispositions, do people assign different risks and utilities to different technologies, are the risk-utility trade-offs comparable for different technologies and can the risk-utility trade-off be quantified. @fig-concept illustrates the general approach.

![General approach of the micro-scenarios: A single survey captures the evaluations of various topics, which are then condidered as topic evaluations and mapped spatially to analyze the relationships between the topics.](figures/concept.png){fig-align="center" #fig-concept}

The main article presents details on this approach and outlines how to design and analyse studies. You can find and cite the main article here:

-   Brauner, P. et al. (2024), Visual Maps of Technology Acceptance -- Spatial Mapping of Factors Influencing Social Acceptance of Technologies Using Micro Scenarios, XXX

This notebook exemplifies the calculation of data for the the two different research perspectives (perspective 1: as user factor and perspective 2: as topic factor) of the micro scenario approach using R. Of course, all transformation can equally be done with other software.

For this example, we use synthetic data that is simulated but similar to real survey data. This makes it easier to follow our approach, as the data has not to be cleaned from unnecessary variables or erroneous inputs form participants. Further, the data clearly follows the pre-specified properties. The creation of the synthetic data is presented in the companion notebook in the same folder.

The remainder of this notebook is structured as follows: First, we load the required packages, then we create synthetic data that we use as our input (replace this with your real data). Second, we transform the data into the long format (see, for example, <https://tidyr.tidyverse.org/reference/pivot_longer.html>), then we analyse the data as a user factor (research perspective 1), and then as a topic factor (research perspective 2); including a visualization of the outcomes.

# Preparation

## Load required libraries

For the actual analysis, we build on the `tidyverse` and `ggplot` packages. For creating the synthetic data, we further use the `faux` package that allows the generation of data based on specific properties (e.g., pre-defined means and correlations between the variables).

```{r LOADLIBRARIES}
library(tidyverse)
library(scales)  # format_percent
library(ggplot2) # graphics
library(ggrepel) # label placement in the scatter plot
library(knitr)   # Tables
```

## Load Data

For this demo, we will load synthetic data that simulates the properties from real survey data.
@fig-surveysdatastructure illustrates the layout of a typical data set from survey tools, where each row represents the responses from a single participant.

![Illustration of typical data from a survey with the micro scenario approach with user demographics, additional user factors, and the topic evaluations.](figures/surveydata.png){fig-align="center" #fig-surveysdatastructure}

The structure of the data is very similar to the data export from the survey tool Qualtrics. The creation of the synthetic data is documented in the companion notebook in the same folder.

```{r LOADDATA}
  data <- readRDS("syntheticdata.rds")
```
The structure of the data looks as follows:
```{r PRINTSTRUCTURE, echo=FALSE}
  str(data)
```

The loaded data set contains several variables: First, a unique user identifier (id), an user variable (e.g., attitude towards a topic), then an arbitrary number of topic assessments (N in our example) with variables for each evaluation dimension (two in this example) for each topic. As an example, we use *perceived risk* and *perceived utility* for the topic evaluations. Of course, one can use different or more evaluation dimensions (see article).

The variables for the topic evaluations follow a standardized naming scheme, i.e., `a01_matrix_02`, where `01` stands for the ID of the queried topic, `02` for the queried evaluation dimension, and `matrix` for the name of the variable block in the survey tool. That is the naming scheme Quartics uses.

# Analysis of the data

After the (synthetic) survey data is loaded to the variable `data`, we now start the actual analysis.

## Setup

First, read the list of queried topics and their labels from a `.csv` file (can easily be adjusted to your needs).
Second, define queried evaluation dimensions. Here, we have a vector of two dimensions, but one can define more depending on your research questions and survey.

```{r LOADLABELS}
TOPICS <- read.csv2("matrixlabels.csv") 
DIMENSIONS = c("risk", "utility")  
```

## Long Format

Next, the topic evaluations from the survey data is transformed into the long format using `pivot_longer` (one row with a single value for each participant, topic, and evaluation dimension; one row per observation). Hereto, we use that the variables for the topic evaluations in the original data table have a systematic naming convention (see above).

The resulting data set contains a participant identifier, identifier for the topic and the evaluation dimension, and lastly a column for the value. We use this format as the foundation for the later transformation steps.

```{r SURVEYTOLONG}
evaluationsLong <- data %>% 
  # selects columns id and "aNUMBER_matrix_NUMBER" (scheme from loop&merge)
  dplyr::select(id, matches("a\\d+\\_matrix\\_\\d+")) %>%
  tidyr::pivot_longer(
    cols = matches("a\\d+\\_matrix\\_\\d+"), # topics and their evaluations
    names_to = c("question", "dimension"),
    names_pattern = "(.*)_matrix_(.*)",   # Separate topic ID and evaluation ID 
    values_to = "value",
    values_drop_na = FALSE) %>%
    dplyr::mutate( dimension = as.numeric(dimension) ) %>%
    dplyr::mutate( dimension = DIMENSIONS[dimension]) %>%  # change to readable dimension names
    dplyr::mutate( value = -(((value - 1)/3) - 1))  # rescale value from [ 1...7 ] to [ -100%...100% ]

# Recode some of the evaluation dimensions if necessary
evaluationsLong <- evaluationsLong %>%
  dplyr::mutate( value = if_else(dimension!="risk", value, -value))
```

## Perspective 1: As user factor

TODO: WARUM BRAUCHE ICH DAS.
Microszenarien sind messwiederholung. disposition. unabhöngig von der konkreten topic.


The first perspective offers a straight forward view on the data: For each evaluation dimension (e.g., *risk* and *utility*), we calculate average scores across all queried topics. These scores can be considered as a user factor or individual differences, e.g., one can study if these overall attributions differ between the participants and if they relate to other queried user factors (e.g, if the average risk attributed to all topics relates to a general disposition to risk measured using other psychometric scales).

We then re-join these user factors with the original data using, for example, `dplyr::join_left()`. Afterwards, the calculated average evaluations can be considered as individual differences and related with other user factors from the survey.

```{r PERSPECTIVE1_USERFACTOR}
evaluationByParticipant <- evaluationsLong %>%
  tidyr::pivot_wider(names_from = dimension, values_from = value) %>%
  dplyr::group_by(id) %>%
  dplyr::summarize(
    across(
      all_of( DIMENSIONS ),  # Select only evaluation dimensions
      list( mean = ~mean(., na.rm = TRUE),
#            median = ~median(., na.rm = TRUE),
            sd = ~sd(., na.rm = TRUE)),
      .names = "{.col}_{.fn}"  # Scheme to define column names
    ), .groups="drop"
  ) %>%
  dplyr::left_join(data, by="id")
```

#### Example research questions:

How is the average perceived *risk* of the participants?

```{r PERSPECTIVE1_VARIABLESUMMARYRISK}
summary(evaluationByParticipant$risk_mean)
```

How is the average perceived *utility* of the participants?

```{r PERSPECTIVE1_VARIABLESUMMARYUZTILITY}
summary(evaluationByParticipant$utility_mean)
```

Does *uservariable* from the survey correlate to *risk*?

```{r PERSPECTIVE1_USERVARIABLERISK}
cor.test(evaluationByParticipant$risk_mean,
    evaluationByParticipant$uservariable)
```

Does uservariable correlate with the average *perceived utility*?

```{r PERSPECTIVE1_USERVARIABLEUTILITY}
cor.test(evaluationByParticipant$utility_mean,
    evaluationByParticipant$uservariable)
```

## Perspective 2: Topic factors

TODO: WARUM BRAUCHE ICH DAS.

[ ] Konstentenz: Wechselt von Valenz und Expectancy. auf Risk und Utility.
=> Besser nur Risk und Utility.

[ ] Rustikale Grafik 3.

[ ] Median weglassen. Kommt plötzlich und nur einmal.

[ ] (outlier) -> (absichtlicher deliberate outlier)

[ ] Regressionsgrade hinter die Punkte.

[ ] darstellungsorm erinnert an das kanu-modell - basismerkmal, 


Next, we delve into the analysis of the topic evaluations. First, we report the average evaluations (e.g, *risk* and *utility*) across all queried topics.

### Calculate Average evaluations

Based on the long format, we group by evaluation dimension and aggregate across all topics and participants. Note: For full sampling, the results are the same as for perspective 1 (see above).
@tbl-dimensions and @fig-evaluations show the result of this calculation.

```{r PERSPECTIVE2_OVERVIEW}
# MEAN and SD of all evaluation dimensions across all queried topics
evaluationByDimension <- evaluationsLong %>%
  dplyr::group_by( dimension ) %>%
  dplyr::summarise( mean = mean(value, na.rm = TRUE),
                    sd = sd(value, na.rm = TRUE),
                    .groups="drop")
```

```{r TABLEDIMENSIONS, echo=FALSE}
#| label: tbl-dimensions
#| tbl-cap: Averages for each evaluation dimension across all queried topics and across all participants.
knitr::kable(evaluationByDimension)
```

```{r PERSPECTIVE2_PLOTOVERVIEW}
#| label: fig-evaluations
#| fig-cap: "Mean evaluation across all topics and aggregated across all participants."
overallDimension <- ggplot(evaluationByDimension,
  aes(x = dimension, y = mean)) + 
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = percent_format(),
                     limits=c( -1, +1 )) +
  labs(x = "Evaluation Dimension",
       y = "Values",
       title = "Average Evaluation across all Dimensions and Participants")
overallDimension
```

### Prepare Individual Topics

We now calculate the average evaluations for each topic across all participants. The resulting data frame has N rows for the number of topics we have queried and rows for the arithmetic mean and standard deviation for each evaluated dimension (e.g., *risk* and *utility*). Lastly, we attach the labels for each topic using `dplyr::left_join()`. @fig-topicdatastructure shows how the resulting data is structured.

![Resulting data format for the evaluation of the topics. Each row stores the mean evaluation (and its dispersion) of a topic. This data can then be further analysed.](figures/topicdata.png){fig-align="center" #fig-topicdatastructure}

The output can then be tabulated, sorted or filtered by highest/lowest evaluations, or visualized.
@tbl-topics shows the unsorted/unfiltered result.


```{r PERSPECTIVE2_TOPICFACTOR}
evaluationByTopic <-  evaluationsLong %>%
  tidyr::pivot_wider(
    names_from = dimension,
    values_from = value) %>%
  dplyr::group_by( question ) %>%
  dplyr::summarize(
    across(
      all_of( DIMENSIONS ),  # Select the variables from var_names
      list(mean = ~mean(., na.rm = TRUE),
      	   sd = ~sd(., na.rm = TRUE)),
      .names = "{.col}_{.fn}"    # Define column names for the results
    ), .groups="drop"
  ) %>%
  dplyr::left_join(TOPICS, by="question") # attach question labels
```

```{r TABLETOPICS, echo=FALSE}
#| label: tbl-topics
#| tbl-cap: Average evaluation of the queried topics.
knitr::kable(
  evaluationByTopic %>%
    dplyr::select(-question, -shortlabel) %>%
    dplyr::relocate(label))
```

### Topic Correlations

Next, we analyse the correlation between the evaluation dimensions across the topics. In the example in @tbl-topiccorrelations, we study if the attribute *risk* is related to the attributed *utility* for the different topics under investigation.
In this example, we have only two target variables for the topic evaluations.
With more variables, one can also do more complex analyses, such as if a linear model with *risk* and *utility* explains the overall *valence* towards the queried topics.

Note: We do not analyse the individual differences of the participants, but the correlations between the topics as attributed by the participants.

```{r TOPICCORRELATIONS}
#| label: tbl-topiccorrelations
#| tbl-cap: Correlations between the evaluation dimensions across all topics
evaluationByTopic %>%
  dplyr::select(ends_with("_mean")) %>%
  correlation::correlation() %>%
 kable()
```

### Visualize the Topics

Lastly, the results are visualized as a scatter plot.
The plot in @fig-topicscatterplot facilitates the visual identification of the spread of the topics on a spatial map defined by the evaluation dimension, if there is (linear) relationship between the queried evaluation dimensions of the topics, the slope and intercept of that relationship, and if some topics have very different evaluations than the others (outliers).

```{r PERSPECTIVE2_EXAMPLEPLOT}
#| label: fig-topicscatterplot
#| fig-cap: "Scatter plot of the evaluations of the micro scenarios."
#| warning: false

scatterPlot <- evaluationByTopic %>%
  ggplot( aes( x = risk_mean,
               y = utility_mean,
               label = shortlabel)) + 
  coord_cartesian(clip = "on") +
  geom_vline(xintercept = 0, size = 0.25, color="black", linetype=1) + 
  geom_hline(yintercept = 0, size = 0.25, color="black", linetype=1) + 
  # diagonal line indicating where both dimensions are congruent 
  annotate("segment",
           x = -1, y = +1,
           xend = +1, yend = -1,
           colour = "black",
           linewidth = 0.25,
           linetype = 2) +
  # Annotate the quadrants
  geom_label(aes(x = -1, y = -1, label = "LOW RISK & LOW UTILITY"),
             vjust = "middle", hjust = "inward",
             size = 1.75,
             label.size = NA, color="black", fill = "#c7ddf2") +
  geom_label(aes(x = -1, y = +1, label = "LOW RISK & HIGH UTILITY"),
             vjust = "middle", hjust = "inward",
             size = 1.75,
             label.size = NA, color="black", fill = "#c7ddf2") + 
  geom_label(aes(x = +1, y = -1, label = "HIGH RISK & LOW UTILITY"),
             vjust = "middle", hjust = "inward",
             size = 1.75,
             label.size = NA, color="black", fill = "#c7ddf2") + 
  geom_label(aes(x = +1, y = +1, label = "HIGH RISK & HIGH UTILITY"),
             vjust = "middle", hjust = "inward",
             size = 1.75,
             label.size = NA, color="black", fill = "#c7ddf2") + 
  # add the labels...
  geom_label_repel(
    max.time = 3,
    color = "black",
    fill = "gray95",
    force_pull   = 0,
    max.overlaps = Inf,
    ylim = c(-Inf, Inf),
    xlim = c(-Inf, Inf),
    segment.color ="#00549f",
    segment.size = 0.25,
    min.segment.length = 0,
    size = 2.5,
    label.size = NA,
    label.padding = 0.105,
    box.padding = 0.125
  ) +
  geom_smooth(method = "lm", se = TRUE) +
  geom_point() +      # geom for the data points
  labs( title = "Illustration of the risk-utility tradeoff ...",
        caption = "Based on synthetic data for illustrative purposes. See linked companion notebook under https://osf.io/96ep5/",
        x = "AVERAGE ESTIMATED RISK\n(without risk — very risky)",
        y = "AVERAGE ESTIMATED UTILITY\n(useless — useful)") +
  scale_x_continuous(labels = percent_format(), limits=c( -1, +1 )) +
  scale_y_continuous(labels = percent_format(), limits=c( -1, +1 ))
scatterPlot

ggsave("simulatedriskutility.pdf",
       plot = scatterPlot,
       width = 8, height = 6,
       units = "in")
```

# Closing remarks

This notebook demonstrates how surveys based on the micro-scenario approach can be analysed and visualized. It provides runnable code for studying both research perspectives (individual differences and topic evaluation) that can be adapted to fit your own survey and data.
Please pay particular attention to the correct coding and polarization of the input variables.

Please be aware of the limitations of this approach (e.g., when are point estimations admissible, can biased results occur due to to biased sampling of topics) and consult the main articles for further reference and mitigation strategies.

# Acknowledgements

This approach has evolved over a longer period of time and through several research projects. We would like to thank all those who have directly or indirectly, consciously or unconsciously, inspired us to take a closer look at this approach and who have given us the opportunity to apply this approach in various contexts.
Special thanks to Julian Hildebrandt for feedback on an earlier version of this notebook.

Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy — EXC- 2023 Internet of Production — 390621612.
