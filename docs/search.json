[
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Example Notebook to Analyse and Visualize Data from a Micro Scenario-based Study",
    "section": "",
    "text": "The micro-scenario approach simplifies measuring people’s opinions on different topics. It connects these opinions to individual user factors (research perspective 1), ranks the topics, and creates a visual map to pinpoint conflicting issues (research perspective 2), all within a single survey.\nFor instance, consider analysing risk-utility trade-offs among various technologies: Do individuals attribute varying risks and utilities to distinct technologies? Are people predisposed to different risk or utility perceptions? Is the comparability of risk-utility trade-offs consistent across different technologies, and can these trade-offs be quantified? Figure 1 illustrates the overall approach.\n\n\n\nFigure 1: The micro-scenario approach involves consolidating evaluations of diverse topics in a single survey. These evaluations are treated as topic assessments and spatially mapped to analyze the interrelationships among them.\n\n\nThe main article provides comprehensive insights into this approach and outlines the methodology for designing and analyzing studies. You can locate and cite the main article here:\n\nVisual Cognitive Mapping: Assessing Social Acceptance of Emerging Technologies through Micro Scenarios, Philipp Brauner & Julia Offermann (2024)\n\nThis notebook demonstrates the calculation of data for the two research perspectives of the micro-scenario approach (Perspective 1: user factor and Perspective 2: topic factor) using R. Note that all transformations and calculations can also be performed using other software.\nIn this example, we utilize synthetic data generated to resemble real survey data. This choice simplifies the follow-through of our approach, eliminating the need for cleaning the data from irrelevant variables or erroneous participant inputs. Additionally, the synthetic data adheres to pre-specified properties. The creation of the synthetic data is detailed in the companion notebook within the same folder.\nThe rest of this notebook is organized as follows: Firstly, we load the necessary packages, followed by loading the synthetic data as our input (replace this with your actual data). Secondly, we transform the data into the long format (refer to, for instance, https://tidyr.tidyverse.org/reference/pivot_longer.html), proceed to analyse the data as a user factor (research perspective 1), and subsequently as a topic factor which includes visualizing the outcomes (research perspective 2).\nAcknowledgements:\nThis approach evolved over time and through several research projects.\nI would like to thank all those who have directly or indirectly, consciously or unconsciously, inspired me to take a closer look at this approach and who have given me the opportunity to apply this approach in various contexts. In particular, I would like to thank:\nJulia Offermann, for indispensable discussions about this approach and so much encouragement and constructive comments during the last meters of the manuscript.\nRalf Philipsen, without whom the very first study with that approach would never have happened, as we developed the crazy idea to explore the benefits of barriers of using questions in Limesurvey.\nMartina Ziefle for igniting scientific curiosity and motivating me to embark on a journey of boundless creativity and exploration.\nFelix Glawe, Luca Liehner, and Luisa Vervier for working on a study that took this concept to another level.\nJulian Hildebrandt for in-depth discussions on the approach and for validating the accompanying code.\nTim Schmeckel for feedback on the draft of this article.\nThroughout the process I received feedback from reviewers that helped to question this approach and improve the foundation of this approach.\nNo scientific method of the social sciences alone will fully answer all of our questions.\nWe hope that this method provides a fresh perspective on exciting and relevant questions.\nFunded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC- 2023 Internet of Production – 390621612."
  },
  {
    "objectID": "analysis.html#load-required-libraries",
    "href": "analysis.html#load-required-libraries",
    "title": "Example Notebook to Analyse and Visualize Data from a Micro Scenario-based Study",
    "section": "Load required libraries",
    "text": "Load required libraries\nIn our analysis, we mainly use the tidyverse and ggplot packages.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(scales)  # format_percent\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(ggplot2) # graphics\nlibrary(ggrepel) # label placement in the scatter plot\nlibrary(knitr)   # Tables"
  },
  {
    "objectID": "analysis.html#load-data",
    "href": "analysis.html#load-data",
    "title": "Example Notebook to Analyse and Visualize Data from a Micro Scenario-based Study",
    "section": "Load Data",
    "text": "Load Data\nIn this demonstration, we will load the synthetic data that emulates the properties found in real survey data. The other notebook demonstrates the creation of the synthetic data. Figure 2 illustrates the structure of a standard dataset from survey tools, where each row represents the responses from an individual participant.\n\n\n\nFigure 2: Illustration of typical survey data utilizing the micro-scenario approach, featuring user demographics, additional user factors, and topic evaluations.\n\n\nThe data structure closely resembles the data export from the Qualtrics survey tool. The process of generating synthetic data is documented in the companion notebook within the same folder.\n\n  data &lt;- readRDS(\"syntheticdata.rds\")\n\nThe structure of the data looks as follows:\n\n\n'data.frame':   100 obs. of  26 variables:\n $ id          : chr  \"fakeparticipantid-1\" \"fakeparticipantid-2\" \"fakeparticipantid-3\" \"fakeparticipantid-4\" ...\n $ uservariable: num  8.44 7.14 10.57 12.38 5.78 ...\n $ a1_matrix_1 : num  1 2 2 2 2 1 2 2 2 3 ...\n $ a2_matrix_1 : num  2 2 1 2 1 1 1 3 2 3 ...\n $ a3_matrix_1 : num  2 2 3 2 2 3 2 1 3 3 ...\n $ a4_matrix_1 : num  2 2 3 2 2 2 3 2 2 3 ...\n $ a5_matrix_1 : num  3 2 3 3 1 3 3 3 3 3 ...\n $ a6_matrix_1 : num  3 3 3 4 2 3 4 4 4 3 ...\n $ a7_matrix_1 : num  3 3 3 4 3 3 4 3 5 5 ...\n $ a8_matrix_1 : num  5 3 4 4 3 3 5 3 4 6 ...\n $ a9_matrix_1 : num  5 3 5 4 3 4 4 4 5 5 ...\n $ a10_matrix_1: num  5 5 5 6 5 4 5 5 6 6 ...\n $ a11_matrix_1: num  4 5 5 5 5 4 4 6 5 5 ...\n $ a12_matrix_1: num  4 5 5 6 4 5 4 6 5 6 ...\n $ a1_matrix_2 : num  1 3 2 3 3 3 3 1 3 3 ...\n $ a2_matrix_2 : num  1 3 1 2 2 2 2 2 3 3 ...\n $ a3_matrix_2 : num  1 4 2 1 3 2 2 1 2 2 ...\n $ a4_matrix_2 : num  2 3 2 2 3 2 2 2 2 2 ...\n $ a5_matrix_2 : num  4 5 5 5 6 5 5 4 4 6 ...\n $ a6_matrix_2 : num  2 4 3 3 3 3 3 3 2 3 ...\n $ a7_matrix_2 : num  3 4 3 2 4 3 3 2 3 2 ...\n $ a8_matrix_2 : num  2 4 3 2 4 4 2 2 3 3 ...\n $ a9_matrix_2 : num  4 4 4 4 4 3 4 3 4 4 ...\n $ a10_matrix_2: num  4 4 3 3 5 4 4 3 5 3 ...\n $ a11_matrix_2: num  3 4 4 4 5 4 5 3 4 5 ...\n $ a12_matrix_2: num  3 4 4 4 5 4 3 4 3 5 ...\n\n\nThe loaded dataset has various variables. Initially, there’s a unique user identifier (id), followed by a user variable (e.g., attitude towards a topic). Subsequently, there are an arbitrary number of topic assessments (N in our example) with variables for each evaluation dimension. In this instance, we use perceived risk and perceived utility as examples for the topic evaluations. However, one can employ different or additional evaluation dimensions (as detailed in the article).\nThe variables for the topic evaluations adhere to a standardized naming scheme, i.e., a01_matrix_02, where 01 denotes the ID of the queried topic, 02 represents the queried evaluation dimension, and matrix stands for the name of the variable block in the survey tool. This naming scheme is employed by Quartics."
  },
  {
    "objectID": "analysis.html#setup",
    "href": "analysis.html#setup",
    "title": "Example Notebook to Analyse and Visualize Data from a Micro Scenario-based Study",
    "section": "Setup",
    "text": "Setup\nFirstly, read the list of queried topics and their labels from a .csv file (adjustable based on your needs). Secondly, define the queried evaluation dimensions. In this instance, we have a vector of two dimensions, but one can define more based on your research questions and survey structure.\n\nTOPICS &lt;- read.csv2(\"matrixlabels.csv\") \nDIMENSIONS = c(\"risk\", \"utility\")"
  },
  {
    "objectID": "analysis.html#long-format",
    "href": "analysis.html#long-format",
    "title": "Example Notebook to Analyse and Visualize Data from a Micro Scenario-based Study",
    "section": "Long Format",
    "text": "Long Format\nNext, the topic evaluations from the survey data is transformed into the long format using pivot_longer (one row with a single value for each participant, topic, and evaluation dimension; one row per observation). Hereto, we use that the variables for the topic evaluations in the original data table have a systematic naming convention (see above).\nThe resulting data set contains a participant identifier, identifier for the topic and the evaluation dimension, and lastly a column for the value. We use this format as the foundation for the later transformation steps.\n\nevaluationsLong &lt;- data %&gt;% \n  # selects columns id and \"aNUMBER_matrix_NUMBER\" (scheme from loop&merge)\n  dplyr::select(id, matches(\"a\\\\d+\\\\_matrix\\\\_\\\\d+\")) %&gt;%\n  tidyr::pivot_longer(\n    cols = matches(\"a\\\\d+\\\\_matrix\\\\_\\\\d+\"), # topics and their evaluations\n    names_to = c(\"question\", \"dimension\"),\n    names_pattern = \"(.*)_matrix_(.*)\",   # Separate topic ID and evaluation ID \n    values_to = \"value\",\n    values_drop_na = FALSE) %&gt;%\n    dplyr::mutate( dimension = as.numeric(dimension) ) %&gt;%\n    dplyr::mutate( dimension = DIMENSIONS[dimension]) %&gt;%  # change to readable dimension names\n    dplyr::mutate( value = -(((value - 1)/3) - 1))  # rescale value from [ 1...7 ] to [ -100%...100% ]\n\n# Recode some of the evaluation dimensions if necessary\nevaluationsLong &lt;- evaluationsLong %&gt;%\n  dplyr::mutate( value = if_else(dimension!=\"risk\", value, -value))"
  },
  {
    "objectID": "analysis.html#perspective-1-as-user-factor",
    "href": "analysis.html#perspective-1-as-user-factor",
    "title": "Example Notebook to Analyse and Visualize Data from a Micro Scenario-based Study",
    "section": "Perspective 1: As user factor",
    "text": "Perspective 1: As user factor\nThe initial perspective provides a straightforward view of the data. The different presented scenarios serve as a basis for the repeated measurement of the same latent construct and the resulting score can be interpreted as a user factor (or individual differences).\nFor each evaluation dimension (e.g., risk and utility), we compute average scores across all queried topics. Using these cores one can, for instance, investigate if the overall attributions differ among participants or if they correlate with other queried user factors. For example, exploring if the average risk attributed to all topics relates to a general disposition to risk measured using other psychometric scales.\nSubsequently, we rejoin these user factors with the original data using, for instance, dplyr::left_join(). Afterwards, the calculated average evaluations can be regarded as individual differences and correlated with other user factors obtained from the survey.\n\nevaluationByParticipant &lt;- evaluationsLong %&gt;%\n  tidyr::pivot_wider(names_from = dimension, values_from = value) %&gt;%\n  dplyr::group_by(id) %&gt;%\n  dplyr::summarize(\n    across(\n      all_of( DIMENSIONS ),  # Select only evaluation dimensions\n      list( mean = ~mean(., na.rm = TRUE),\n#            median = ~median(., na.rm = TRUE),\n            sd = ~sd(., na.rm = TRUE)),\n      .names = \"{.col}_{.fn}\"  # Scheme to define column names\n    ), .groups=\"drop\"\n  ) %&gt;%\n  dplyr::left_join(data, by=\"id\")\n\n\nExample research questions:\nHow is the average perceived risk of the participants?\n\nsummary(evaluationByParticipant$risk_mean)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.61111 -0.25000 -0.13889 -0.14639 -0.02778  0.19444 \n\n\nHow is the average perceived utility of the participants?\n\nsummary(evaluationByParticipant$utility_mean)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.05556  0.18750  0.27778  0.28667  0.38889  0.61111 \n\n\nDoes uservariable from the survey correlate to risk?\n\ncor.test(evaluationByParticipant$risk_mean,\n    evaluationByParticipant$uservariable)\n\n\n    Pearson's product-moment correlation\n\ndata:  evaluationByParticipant$risk_mean and evaluationByParticipant$uservariable\nt = 6.0174, df = 98, p-value = 3.072e-08\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3596965 0.6495649\nsample estimates:\n      cor \n0.5194173 \n\n\nDoes uservariable correlate with the average perceived utility?\n\ncor.test(evaluationByParticipant$utility_mean,\n    evaluationByParticipant$uservariable)\n\n\n    Pearson's product-moment correlation\n\ndata:  evaluationByParticipant$utility_mean and evaluationByParticipant$uservariable\nt = 2.1082, df = 98, p-value = 0.03757\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.01237368 0.38879757\nsample estimates:\n      cor \n0.2082856"
  },
  {
    "objectID": "analysis.html#perspective-2-topic-factors",
    "href": "analysis.html#perspective-2-topic-factors",
    "title": "Example Notebook to Analyse and Visualize Data from a Micro Scenario-based Study",
    "section": "Perspective 2: Topic factors",
    "text": "Perspective 2: Topic factors\nNext, we delve into the analysis of topic evaluations: Instead of looking at how individual participants perceive the topics as a whole, only all test subject assessments are assigned to the respective topics, for example in order to be able to rank the technologies in terms of the evaluation dimensions. We start with reporting the average evaluations (e.g., risk and utility) across all queried topics.\n\nCalculate Average evaluations\nUsing the long format, we group by evaluation dimension and aggregate across all topics and participants. Note: For a complete sample, the results are equivalent to perspective 1 (see above). Table 1 and Figure 3 show the outcome of this calculation.\n\n# MEAN and SD of all evaluation dimensions across all queried topics\nevaluationByDimension &lt;- evaluationsLong %&gt;%\n  dplyr::group_by( dimension ) %&gt;%\n  dplyr::summarise( mean = mean(value, na.rm = TRUE),\n                    sd = sd(value, na.rm = TRUE),\n                    .groups=\"drop\")\n\n\n\n\n\nTable 1: Averages for each evaluation dimension across all queried topics and across all participants.\n\n\ndimension\nmean\nsd\n\n\n\n\nrisk\n-0.15\n0.46\n\n\nutility\n0.29\n0.38\n\n\n\n\n\n\n\noverallDimension &lt;- ggplot(evaluationByDimension,\n  aes(x = dimension, y = mean)) + \n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(labels = percent_format(),\n                     limits=c( -1, +1 )) +\n  labs(x = \"Evaluation Dimension\",\n       y = \"Values\",\n       title = \"Average Evaluation across all Dimensions and Participants\")\noverallDimension\n\n\n\n\nFigure 3: Mean evaluation across all topics and aggregated across all participants.\n\n\n\n\n\n\nPrepare Individual Topics\nNow, we compute the average evaluations for each topic across all participants. The resulting data frame contains N rows for the number of topics queried and rows for the arithmetic mean and standard deviation for each evaluated dimension (e.g., risk and utility). Finally, we associate labels with each topic using dplyr::left_join(). Figure 4 illustrates the structure of the resulting data.\n\n\n\nFigure 4: The resulting data format displays the evaluation of topics. Each row contains the mean evaluation (along with its dispersion) for a specific topic. This structured data can be subjected to further analysis.\n\n\nThe output can be tabulated, sorted, or filtered based on highest/lowest evaluations, and visualized. Table 2 displays the unsorted and unfiltered results.\n\nevaluationByTopic &lt;-  evaluationsLong %&gt;%\n  tidyr::pivot_wider(\n    names_from = dimension,\n    values_from = value) %&gt;%\n  dplyr::group_by( question ) %&gt;%\n  dplyr::summarize(\n    across(\n      all_of( DIMENSIONS ),  # Select the variables from var_names\n      list(mean = ~mean(., na.rm = TRUE),\n           sd = ~sd(., na.rm = TRUE)),\n      .names = \"{.col}_{.fn}\"    # Define column names for the results\n    ), .groups=\"drop\"\n  ) %&gt;%\n  dplyr::left_join(TOPICS, by=\"question\") # attach question labels\n\n\n\n\n\nTable 2: Average evaluation of the queried topics.\n\n\n\n\n\n\n\n\n\nlabel\nrisk_mean\nrisk_sd\nutility_mean\nutility_sd\n\n\n\n\nTopic 1\n-0.73\n0.25\n0.61\n0.27\n\n\nTopic 10\n0.31\n0.27\n0.15\n0.28\n\n\nTopic 11\n0.35\n0.24\n0.04\n0.24\n\n\nTopic 12\n0.43\n0.27\n-0.09\n0.24\n\n\nTopic 2\n-0.63\n0.25\n0.66\n0.25\n\n\nTopic 3\n-0.51\n0.25\n0.69\n0.22\n\n\nTopic 4\n-0.49\n0.24\n0.47\n0.22\n\n\nTopic 5 (deliberate outlier)\n-0.30\n0.26\n-0.18\n0.27\n\n\nTopic 6\n-0.24\n0.24\n0.39\n0.25\n\n\nTopic 7\n-0.08\n0.29\n0.36\n0.24\n\n\nTopic 8\n0.01\n0.28\n0.29\n0.26\n\n\nTopic 9\n0.13\n0.28\n0.04\n0.22\n\n\n\n\n\n\n\n\nTopic Correlations\nNext, we analyse the correlation between the evaluation dimensions across the topics. In the example in Table 3, we investigate if the attribute risk is related to the attributed utility for the different topics under consideration. In this example, we have only two target variables for the topic evaluations. With more variables, more complex analyses become possible: Such as determining if and to what degree a linear model with risk and utility explains the overall valence towards the queried topics.\nNote: Our analysis focuses on the correlations between the topics as attributed by the participants, rather than individual differences among participants.\n\nevaluationByTopic %&gt;%\n  dplyr::select(ends_with(\"_mean\")) %&gt;%\n  correlation::correlation() %&gt;%\n kable()\n\n\n\nTable 3: Correlations between the evaluation dimensions across all topics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\nt\ndf_error\np\nMethod\nn_Obs\n\n\n\n\nrisk_mean\nutility_mean\n-0.7590283\n0.95\n-0.9284779\n-0.3280107\n-3.686707\n10\n0.0041996\nPearson correlation\n12\n\n\n\n\n\n\n\n\nVisualize the Topics\nFinally, the results are presented through a scatter plot. The plot in Figure 5 allows for the visual identification of the dispersion of topics on a spatial map defined by the evaluation dimension. It helps assess if there is a (linear) relationship between the queried evaluation dimensions of the topics, the slope and intercept of that relationship, and if some topics exhibit significantly different evaluations compared to others (outliers).\n\nscatterPlot &lt;- evaluationByTopic %&gt;%\n  ggplot( aes( x = risk_mean,\n               y = utility_mean,\n               label = shortlabel)) + \n  coord_cartesian(clip = \"on\") +\n  geom_vline(xintercept = 0, size = 0.25, color=\"black\", linetype=1) + \n  geom_hline(yintercept = 0, size = 0.25, color=\"black\", linetype=1) + \n  # diagonal line indicating where both dimensions are congruent \n  annotate(\"segment\",\n           x = -1, y = +1,\n           xend = +1, yend = -1,\n           colour = \"black\",\n           linewidth = 0.25,\n           linetype = 2) +\n  # Annotate the quadrants\n  geom_label(aes(x = -1, y = -1, label = \"LOW RISK & LOW UTILITY\"),\n             vjust = \"middle\", hjust = \"inward\",\n             size = 1.75,\n             label.size = NA, color=\"black\", fill = \"#c7ddf2\") +\n  geom_label(aes(x = -1, y = +1, label = \"LOW RISK & HIGH UTILITY\"),\n             vjust = \"middle\", hjust = \"inward\",\n             size = 1.75,\n             label.size = NA, color=\"black\", fill = \"#c7ddf2\") + \n  geom_label(aes(x = +1, y = -1, label = \"HIGH RISK & LOW UTILITY\"),\n             vjust = \"middle\", hjust = \"inward\",\n             size = 1.75,\n             label.size = NA, color=\"black\", fill = \"#c7ddf2\") + \n  geom_label(aes(x = +1, y = +1, label = \"HIGH RISK & HIGH UTILITY\"),\n             vjust = \"middle\", hjust = \"inward\",\n             size = 1.75,\n             label.size = NA, color=\"black\", fill = \"#c7ddf2\") + \n  # add the labels...\n  geom_label_repel(\n    max.time = 3,\n    color = \"black\",\n    fill = \"gray95\",\n    force_pull   = 0,\n    max.overlaps = Inf,\n    ylim = c(-Inf, Inf),\n    xlim = c(-Inf, Inf),\n    segment.color =\"#00549f\",\n    segment.size = 0.25,\n    min.segment.length = 0,\n    size = 2.5,\n    label.size = NA,\n    label.padding = 0.105,\n    box.padding = 0.125\n  ) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  geom_point() +      # geom for the data points\n  labs( title = \"Illustration of the risk-utility tradeoff ...\",\n        caption = \"Based on synthetic data for illustrative purposes. See linked companion notebook under https://osf.io/96ep5/\",\n        x = \"AVERAGE ESTIMATED RISK\\n(without risk — very risky)\",\n        y = \"AVERAGE ESTIMATED UTILITY\\n(useless — useful)\") +\n  scale_x_continuous(labels = percent_format(), limits=c( -1, +1 )) +\n  scale_y_continuous(labels = percent_format(), limits=c( -1, +1 ))\nscatterPlot\n\nggsave(\"simulatedriskutility.pdf\",\n       plot = scatterPlot,\n       width = 8, height = 6,\n       units = \"in\")\n\n\n\n\nFigure 5: Scatter plot of the evaluations of the micro scenarios."
  },
  {
    "objectID": "synthetic.html",
    "href": "synthetic.html",
    "title": "Creation of Synthetic Data for Demonstrating a Micro Scenario-Analysis and Visualisation",
    "section": "",
    "text": "Introduction\nThis notebook generates synthetic data to demonstrate the analysis of a micro-scenario based study. The corresponding analysis notebook is located in the same folder. For detailed information on this approach and guidance on designing and analysing studies, please refer to the main article. You can find and cite the main article here:.\n\nVisual Cognitive Mapping: Assessing Social Acceptance of Emerging Technologies through Micro Scenarios, Philipp Brauner & Julia Offermann (2024)\n\nThe general concept behind this simulated dataset is to mimic the data export from typical survey software systems, like Qualtrics. However, the data is clean, devoid of additional variables, speeders, or erroneous inputs requiring cleaning. Furthermore, the dataset is structured to exhibit the desired properties of a micro-scenario-based survey, showcasing specific patterns among topics and a defined correlation pattern for evaluation dimensions.\nFor generating synthetic data, we use the faux package, and guidance for this can be found in the package vignette.\n\n\nLoad libraries\nIn the analysis, we rely on the tidyverse and ggplot packages. Additionally, for generating synthetic data with specific properties (e.g., pre-defined means and correlations between variables), we utilize the faux package.\n\nlibrary(faux)    # create simulated data based on given properties\n\n\n************\nWelcome to faux. For support and examples visit:\nhttps://debruine.github.io/faux/\n- Get and set global package options with: faux_options()\n************\n\nlibrary(matrixcalc)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(scales)  # format_percent\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(ggplot2) # graphics\nlibrary(ggrepel) # label placement in the scatter plot\nlibrary(knitr)   # Tables\n\n\n\nCreate Synthetic Data\nIn this section, we generate synthetic data that simulates properties akin to real survey data. Initially, we create a dataset resembling data from the survey tool Qualtrics. Figure Figure 1 illustrates the structure of a standard dataset from survey tools, where each row represents the responses from a single participant.\n\n\n\nFigure 1: Illustration of typical survey data utilizing the micro-scenario approach, incorporating user demographics, additional user factors, and topic evaluations.\n\n\nThe generated dataset will include several variables. Initially, a unique user identifier (id) is assigned, followed by a user variable (e.g., attitude towards a topic). Subsequently, N=12 (adjustable) topic assessments are included, with two variables for each evaluation dimension for each topic. In this instance, perceived risk and perceived utility are used as examples for topic evaluations. However, one can utilize different or additional evaluation dimensions (as detailed in the article).\nFor the synthetic data, we specify \\(N\\) topics and two evaluation dimensions, defining that they should be inversely correlated with an intercept.\n\nN &lt;- 12  # number of topics to simulate\nTOPICS &lt;- read.csv2(\"matrixlabels.csv\") \n\n# Generate mean evaluations A and B, inversly correlated and with some noise, and an intercept\n\nevaluationA &lt;- seq(-0.75, 0.50, length.out = N)\nevaluationB &lt;- seq(-0.75, 0.10, length.out = N)\n\n# Add some random noise\nfor (i in 1:N) {\n  evaluationA[i] &lt;- evaluationA[i] + rnorm(1, mean=0, sd=0.05)\n  evaluationB[i] &lt;- evaluationB[i] - rnorm(1, mean=0, sd=0.05) \n}\n\n# Purposefully defined outlier\nevaluationA[5] &lt;- -1/3\nevaluationB[5] &lt;- +1/4\ncombinedMeans = c(evaluationA, evaluationB)\n\n\n\n\n\n\nFigure 2: Target topic evaluations. Noise due to random sampling will be added in a later step.\n\n\n\n\nThe variables for the topic evaluations must adhere to a standardized naming scheme, i.e., a01_matrix_02, where 01 represents the ID of the queried topic, 02 stands for the queried evaluation dimension, and matrix denotes the name of the variable block in the survey tool. This is the naming scheme used by Quartics.\n\nvarnames &lt;- c(paste0(\"a\", 1:N, \"_matrix_1\"), paste0(\"a\", 1:N, \"_matrix_2\"))\n\nAdditionally, we specify that the evaluations of the first and second evaluation dimensions are strongly correlated within topics and negatively correlated between topics. The data is randomized to approximately, but not exactly, align with this scheme, simulating a scenario as if it were randomly drawn from a population parametrized in this manner.\n\n# Generate correlation matrix with random correlations in the specified range\ngenerate_cor_matrix &lt;- function(n, range = c(0.2, 0.6)) {\n  corr_values &lt;- matrix(runif(n^2, range[1], range[2]), nrow = n)\n  for (i in 1:n) {\n    for(j in 1:n) {\n      corr_values[i,j] = max(-1, min(1, corr_values[i,j])) # check bounds\n    }\n  }\n  for (i in 1:n) {\n    for(j in 1:n) {\n      corr_values[i,j] = corr_values[j,i] # symmetric please\n    }\n  }\n  diag(corr_values) &lt;- 1  # Set diagonal to 1\n  corr_values\n}\n\n# Generating covariance matrices for variables A and B\ncor_matrix_A &lt;- generate_cor_matrix(N)\ncor_matrix_B &lt;- generate_cor_matrix(N, range = c(0.3, 0.5))\n\n# Generating negative correlations between variables A and B\nnegative_corr &lt;- -.25     # r between A and B\ncor_matrix_AB &lt;- matrix(negative_corr, nrow = N, ncol = N)\n\n# Combining covariance matrices for A, A and B, and B\n# Must be symmetric and positive-definite\ncor_matrix &lt;- matrix(0, nrow = 2 * N, ncol = 2 * N)\ncor_matrix[1:N, 1:N] &lt;- cor_matrix_A # upper-left\ncor_matrix[(N + 1):(2 * N), (N + 1):(2 * N)] &lt;- cor_matrix_B # lower-right\ncor_matrix[1:N, (N + 1):(2 * N)] &lt;- cor_matrix_AB \ncor_matrix[(N + 1):(2 * N), 1:N] &lt;- t(cor_matrix_AB) #transposed for neg. covariances\n\n\n\nCheck the validity of the data\nBefore proceeding, let’s perform a check. Why? We require a symmetric and positive-definite matrix to generate random data based on the defined parameters. A correlation matrix is, by definition, symmetric and positive semi-definite. If we encounter a correlation matrix that is not positive semi-definite, it could be due to numerical precision issues or errors in computation, particularly when dealing with very large matrices or near-linear dependencies between variables. If this occurs, repeat the steps above—it should work out after a few tries.\nRequest to the readers: Any suggestions on how to address this issue are appreciated. However, this step is not necessary if you already have real survey data, as it specifically pertains to the creation of synthetic data.\nNow, let’s create the synthetic data based on the previously defined means and correlation parameters. Name the variables following the required schema. We provide population parameters and draw a random sample based on these.\n\ndata &lt;- rnorm_multi(\n  varnames = varnames,\n  n = 100, # sample size, i.e., number of participants in the survey\n  mu = combinedMeans,\n  sd = 0.25,\n  r = cor_matrix,\n  empirical = FALSE\n)\n\n\n\nRecode data\nFinally, we record the data to resemble typical survey data. Firstly, ensure that the bounds are met (e.g., values between \\(-1\\) and \\(+1\\); due to random sampling, this may not always be the case). Next, convert the percent scores to the typical survey data domain, i.e., discrete values from \\(1\\) to \\(7\\).\n\ndata &lt;- as.data.frame(data) %&gt;%\n  dplyr::mutate(across(everything(), ~ ifelse(. &lt; -1, -1, ifelse(. &gt; 1, 1, .)))) %&gt;%\n  dplyr::mutate(across(everything(), ~ (.+1)/2)) %&gt;%\n  dplyr::mutate(across(everything(), ~ (6*.))) %&gt;%\n  dplyr::mutate(across(everything(), ~ 1 + round(.)))\n\nUltimately, let’s add a participant ID and a simulated user variable, calculated from the already simulated data and therefore strongly correlated.\n\ndata &lt;- data %&gt;%\n  dplyr::mutate(id = paste0(\"fakeparticipantid-\", row_number())) %&gt;%\n  dplyr::mutate(uservariable = rnorm_pre(a1_matrix_1+a2_matrix_1, mu = 10, sd = 2, r = 0.5)) %&gt;%\n  dplyr::relocate(id, uservariable)\n\n\n\nSave data\nFinally, we save the generated data to an RDS file.\n\nsaveRDS(data, \"syntheticdata.rds\")\n\nThat file can be used for the actual analysis and visualization (this main notebook utilizes this data).\n\n\n\n\nAcknowledgements:\nThis approach evolved over time and through several research projects.\nI would like to thank all those who have directly or indirectly, consciously or unconsciously, inspired me to take a closer look at this approach and who have given me the opportunity to apply this approach in various contexts. In particular, I would like to thank:\nJulia Offermann, for indispensable discussions about this approach and so much encouragement and constructive comments during the last meters of the manuscript.\nRalf Philipsen, without whom the very first study with that approach would never have happened, as we developed the crazy idea to explore the benefits of barriers of using questions in Limesurvey.\nMartina Ziefle for igniting scientific curiosity and motivating me to embark on a journey of boundless creativity and exploration.\nFelix Glawe, Luca Liehner, and Luisa Vervier for working on a study that took this concept to another level.\nJulian Hildebrandt for in-depth discussions on the approach and for validating the accompanying code.\nTim Schmeckel for feedback on the draft of this article.\nThroughout the process I received feedback from reviewers that helped to question this approach and improve the foundation of this approach.\nNo scientific method of the social sciences alone will fully answer all of our questions.\nWe hope that this method provides a fresh perspective on exciting and relevant questions.\nFunded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC- 2023 Internet of Production – 390621612."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mapping Acceptance: Assessing Emerging Technologies and Concepts through Micro Scenarios",
    "section": "",
    "text": "Introduction\nThe goal of the micro scenario approach is to gather the evaluation of a wide range of topics on few selected response variables and put the different evaluation into context. Hereto, the subjects are presented with a large number of different short scenarios and their evaluation of these is measured using a small set of response variables. The scenario presentation can be a short descriptive text, and/or images, or, in extreme cases, just a single word about an evaluated concept. We call the former “micro scenarios” and the latter “nano scenarios”. The former offers the possibility to briefly explain the evaluated topic whereas the later essentially measures the participants’ affective associations towards a single term.\n\n\n\nFigure 1: Concept of a micro scenario survey compared to a conventional scenario-based survey.\n\n\nEach scenario is evaluated on a the same small set of response items (see Figure 1). As many scenarios are evaluated by the participants, we suggest to use no more than three to five items. With a suitable set of dependent variables, the evaluations offer two different research perspectives: As the first research perspective, they can be understood, as user variables (individual differences between the participants) and correlations between age, gender, or other user factors can be investigated. As the second research perspective, the evaluations serve as topic evaluations and relationships between the evaluation dimensions across the different topics can be studied (differences and communalities between the queried topics). For example, one can explore the relationship between the perceived risk and the perceived utility for a range of different topics or technologies.\nWe illustrate how studies using the micro scenario approach can be analysed and visualized and further we created a page that create synthetic data for illustrating the analysis.\nDetails on this approach, a methodological justification, and practical guidelines can be found in the following article:\n\nVisual Cognitive Mapping: Assessing Social Acceptance of Emerging Technologies through Micro Scenarios, Philipp Brauner & Julia Offermann (2024)\n\n\n\nList of Studies\nSeveral studies based on this approach have been published or are in the making. We document them here, including the research context, sample size, number of topics queried, and dependent variables. If you use this approach, let us know and we can add your study as well.\n\n\n\n\n\nPublication\n#Topics\nDependents\nN\n\n\n\n\nBrauner P, Hick A, Philipsen R and Ziefle M (2023) What does the public think about artificial intelligence? - A criticality map to understand bias in the public perception of AI. Front. Comput. Sci. 5:1113903. doi: 10.3389/fcomp.2023.1113903\n34\nExpectancy, Valence\n122\n\n\n\n\n\n\n\n\n\n\n\nAcknowledgements:\nThis approach evolved over time and through several research projects.\nI would like to thank all those who have directly or indirectly, consciously or unconsciously, inspired me to take a closer look at this approach and who have given me the opportunity to apply this approach in various contexts. In particular, I would like to thank:\nJulia Offermann, for indispensable discussions about this approach and so much encouragement and constructive comments during the last meters of the manuscript.\nRalf Philipsen, without whom the very first study with that approach would never have happened, as we developed the crazy idea to explore the benefits of barriers of using questions in Limesurvey.\nMartina Ziefle for igniting scientific curiosity and motivating me to embark on a journey of boundless creativity and exploration.\nFelix Glawe, Luca Liehner, and Luisa Vervier for working on a study that took this concept to another level.\nJulian Hildebrandt for in-depth discussions on the approach and for validating the accompanying code.\nTim Schmeckel for feedback on the draft of this article.\nThroughout the process I received feedback from reviewers that helped to question this approach and improve the foundation of this approach.\nNo scientific method of the social sciences alone will fully answer all of our questions.\nWe hope that this method provides a fresh perspective on exciting and relevant questions.\nFunded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC- 2023 Internet of Production – 390621612."
  }
]